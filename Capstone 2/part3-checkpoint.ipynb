{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore', category = DeprecationWarning)\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from datetime import datetime\n",
    "import scipy.stats.distributions as dist\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "file = pd.read_csv(r'C:\\Users\\xuanx\\Desktop\\Steam_Reviews\\steam_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['language'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick out english reviews only\n",
    "df = file[file['language'] == 'english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking review column for null values and removing them.\n",
    "print(df.review.isnull().sum())\n",
    "df1 = df[df.review.isnull()== False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['recommended'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis testing: early access population mean versus release reviews means\n",
    "df1.groupby('written_during_early_access')['recommended'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = df1.groupby('written_during_early_access')['recommended'].value_counts(normalize = True)\n",
    "prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df1.groupby('written_during_early_access')['recommended'].count()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_early = prop[(True, False)]\n",
    "prop_not_early = prop[(False, False)]\n",
    "print(prop_early, prop_not_early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_early = n[True]\n",
    "n_not_early = n[False]\n",
    "print(n_early, n_not_early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat = (n_early * prop_early + n_not_early * prop_not_early) / (n_early + n_not_early)\n",
    "std_error = np.sqrt(p_hat * (1-p_hat) / n_early + p_hat * (1-p_hat) / n_not_early)\n",
    "z_score = (prop_early - prop_not_early) / std_error\n",
    "print(z_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_not_recommended = np.array([2644, 2597])\n",
    "n_row = np.array([14424 + 2644, 6717 + 2597])\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "z_score, p_value = proportions_ztest(count = n_not_recommended, nobs = n_row, alternative = 'two-sided')\n",
    "z_score, p_value\n",
    "\n",
    "#Reject null hypothesis.\n",
    "#There is a difference in proportion between not recommended reviews in early access and not recommended reviews outside\n",
    "# of early access at a 1% level of significance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#received for free population reviews versus paid population reviews\n",
    "\n",
    "df1.groupby('received_for_free')['recommended'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#received for free population = to paid population reviews?\n",
    "rec_free_key = np.array([248307, 30925])\n",
    "n_row = np.array([248307+30925, 8319820+1019635])\n",
    "\n",
    "z_score, p_value = proportions_ztest(count = n_not_recommended, nobs = n_row, alternative = 'larger')\n",
    "z_score, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df1\n",
    "df_20_20_samp = df_clean.groupby(['recommended'])\n",
    "df_20_20_samp.size()\n",
    "df_20_20 = df_20_20_samp.apply(lambda x: x.sample(300, replace = False).reset_index(drop=True))\n",
    "df_20_20['recommended'].value_counts()\n",
    "#df_20_20_samp = df_clean.sample(15000, replace = False, random_state = 42)\n",
    "#df_20_20_samp['recommended'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_20['recommended'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_20 = df_20_20.droplevel(['recommended'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_20.review = df_20_20.review.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to lowercase and removing punctuation \n",
    "\n",
    "def remove_punct(txt):\n",
    "    text_input = \"\".join([char for char in txt if char not in string.punctuation])\n",
    "    return text_input\n",
    "\n",
    "df_20_20['token_review'] = df_20_20['review'].apply(lambda x: remove_punct(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numbers\n",
    "\n",
    "def remove_digits(txt):\n",
    "    return re.sub(r' \\d+', '', str(txt))\n",
    "\n",
    "df_20_20['token_review1'] = df_20_20['token_review'].apply(lambda x: remove_digits(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non alphabet characters\n",
    "\n",
    "def remove_chars(txt):\n",
    "    return re.sub(\"[^a-zA-Z]+\", ' ', txt)\n",
    "\n",
    "df_20_20['token_review2'] = df_20_20['token_review1'].apply(lambda x: remove_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert reviews into tokens\n",
    "\n",
    "def tokenize(txt):\n",
    "    tokens = re.split('\\W+', txt)\n",
    "    return tokens\n",
    "\n",
    "df_20_20['token_review3'] = df_20_20['token_review2'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def rem_stopwords(txt):\n",
    "    text = [word for word in txt if word not in stopword]\n",
    "    return text\n",
    "\n",
    "df_20_20['token_review4'] = df_20_20['token_review3'].apply(lambda x: rem_stopwords(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert words to base form \n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(txt):\n",
    "    text = [wn.lemmatize(word) for word in txt]\n",
    "    return text\n",
    "\n",
    "df_20_20['token_review5']=df_20_20['token_review4'].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_20['reviews_fin'] = df_20_20['token_review5'].apply(lambda x:' '.join(x))\n",
    "#df_20_20['rec_var'] = df_20_20['recommended'].apply(lambda x: 0 if x==1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_20_20.reviews_fin.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews = df_20_20[df_20_20['recommended']== 1] #majority: variable positive reviews\n",
    "neg_reviews = df_20_20[df_20_20['recommended']== 0] #minority: variable negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_reviews['review'].count(),\n",
    "    neg_reviews['review'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "pos_rev_words = pos_reviews['token_review5']\n",
    "pos_words = []\n",
    "for reviews in pos_rev_words: \n",
    "    pos_words += reviews\n",
    "    \n",
    "freqdist_pos = FreqDist(pos_words)\n",
    "freqdist_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rev_words = neg_reviews['token_review5']\n",
    "neg_words = []\n",
    "for reviews in neg_rev_words: \n",
    "    neg_words += reviews\n",
    "    \n",
    "freqdist_neg = FreqDist(neg_words)\n",
    "freqdist_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                     background_color = 'white',\n",
    "                     stopwords = stopword,\n",
    "                     min_font_size = 10).generate_from_frequencies(freqdist_pos)\n",
    "\n",
    "plt.figure(figsize = (10,10), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.savefig('postive.png')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                     background_color = 'white',\n",
    "                     stopwords = stopword,\n",
    "                     min_font_size = 10).generate_from_frequencies(freqdist_neg)\n",
    "\n",
    "plt.figure(figsize = (10,10), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.savefig('negative.png')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(nltk.ngrams(pos_words, 2)).value_counts())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(nltk.ngrams(pos_words, 3)).value_counts())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(nltk.ngrams(neg_words, 2)).value_counts())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(nltk.ngrams(neg_words, 3)).value_counts())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_20_20.reset_index().groupby(['recommended']).apply(lambda x: x.sample(frac = 0.8, random_state = 42)\n",
    "                                                             ).reset_index(drop = True).set_index('index')\n",
    "\n",
    "test = df_20_20.drop(train.index)\n",
    "\n",
    "train['recommended'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"reviews_fin\"]\n",
    "y_train = train['recommended'].replace({True: 1, False: 0})\n",
    "X_test = test['reviews_fin']\n",
    "y_test = test['recommended'].replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_train_arr = X_train_vec.toarray()\n",
    "print(X_train_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = vectorizer.transform(X_test)\n",
    "X_test_arr = X_test_vec.toarray()\n",
    "print(X_test_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = X_train_arr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(12, input_shape = (n_words,), activation = 'relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "nn_model.add(Dense(8, activation = 'relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "nn_model.add(Dense(1, activation ='sigmoid'))\n",
    "\n",
    "tensorBoardCallback = TensorBoard(log_dir = './logs', write_graph = True)\n",
    "nn_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "history = nn_model.fit(X_train_arr, y_train, epochs = 10, verbose = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = nn_model.evaluate(X_test_arr, y_test, verbose = 0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history.history['val_accuracy'])\n",
    "plt.legend([\"Train\", \"test\"], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "import logging\n",
    "from sklearn.metrics import recall_score, roc_auc_score, make_scorer, roc_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Pipeline\n",
    "\n",
    "pipe = Pipeline(steps = [(\"vects\", TfidfVectorizer()), (\"classifier\", RandomForestClassifier())])\n",
    "\n",
    "pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary with the hyperparameters\n",
    "search_space = [\n",
    "                {\"classifier\": [RandomForestClassifier()]},\n",
    "                {\"classifier\": [MultinomialNB()]}\n",
    "               ]\n",
    "\n",
    "test_params = [\n",
    "                {\"classifier\": [RandomForestClassifier()],\n",
    "                'classifier__max_depth': [10, 20, 50, 100]}\n",
    "               ]\n",
    "\n",
    "back_up_space = [{\"vects\": [TfidfVectorizer()],\n",
    "                \"vects__ngram_range\": [(1,1), (2,2), (3,3)],\n",
    "                'vects__use_idf': [True, False]},\n",
    "                {\"vects\": [CountVectorizer()],\n",
    "                \"vects__ngram_range\": [(1,1), (2,2), (3,3)]},\n",
    "                {\"classifier\": [RandomForestClassifier()],\n",
    "                'classifier__max_depth': [10, 20, 50, 100],\n",
    "                'classifier__n_estimators': [200, 400, 600, 800],\n",
    "                'classifier__max_depth': [20,30,60,None] },\n",
    "                {\"classifier\": [MultinomialNB()]}\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#create gridsearchcv object (cv=kfold) **check kfold\n",
    "cv = KFold(n_splits=5, shuffle = True)\n",
    "scoring={'AUC': metrics.make_scorer(roc_auc_score, needs_proba = True), 'Accuracy': metrics.make_scorer(metrics.accuracy_score)}\n",
    "grid = GridSearchCV(estimator = pipe, param_grid = search_space, cv = cv, scoring = scoring,\n",
    "                    return_train_score = True, verbose = 1, n_jobs = -1, refit = 'AUC', error_score = 'raise')\n",
    "best_model = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.best_score_, best_model.best_estimator_, best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a df from cv data\n",
    "cv_scores = pd.DataFrame(best_model.cv_results_)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cv_scores['params'].iloc[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict_proba(X_test)[:,1]\n",
    "print(best_model.score(X_test, y_test))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf = RandomForestClassifier()\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,1), use_idf = True)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "clf_model = clf.fit(X_train_vect, y_train)\n",
    "y_test_proba = clf_model.predict_proba(X_test_vect)[:,1]\n",
    "print(roc_auc_score(y_test, y_test_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_features = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "print(train_features.shape, test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle = True)\n",
    "\n",
    "\n",
    "pipe_multi_count = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "pipe_rf_count = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('clf', RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "pipe_multi_tfidf = Pipeline([('vect', TfidfVectorizer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "pipe_rf_tfidf = Pipeline([('vect', TfidfVectorizer()),\n",
    "                      ('clf', RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "\n",
    "grid_params_multi = {'vect__ngram_range': [(1,1), (2,2), (3,3)]\n",
    "                     }\n",
    "\n",
    "grid_params_rf = {'vect__ngram_range': [(1,1), (2,2), (3,3)],\n",
    "                  'clf__max_depth': [10, 20, 50, 100],\n",
    "                  'clf__n_estimators': [200, 500, 800, 1000, 1500]\n",
    "                     }\n",
    "\n",
    "multiNB = GridSearchCV(estimator = pipe_multi_count,\n",
    "                       param_grid = grid_params_multi,\n",
    "                       scoring = 'roc_auc',\n",
    "                       cv = cv,\n",
    "                       \n",
    "                       error_score = 'raise',\n",
    "                       n_jobs = -1)\n",
    "\n",
    "rf = GridSearchCV(estimator = pipe_rf_count,\n",
    "                  param_grid = grid_params_rf,\n",
    "                  scoring = 'roc_auc',\n",
    "                  cv = cv,\n",
    "                  error_score = 'raise',\n",
    "                  n_jobs = -1)\n",
    "\n",
    "multiNB_tfidf = GridSearchCV(estimator = pipe_multi_tfidf,\n",
    "                       param_grid = grid_params_multi,\n",
    "                       scoring = 'roc_auc',\n",
    "                       cv = cv,\n",
    "                       \n",
    "                       error_score = 'raise',\n",
    "                       n_jobs = -1)\n",
    "\n",
    "rf_tfidf = GridSearchCV(estimator = pipe_rf_tfidf,\n",
    "                  param_grid = grid_params_rf,\n",
    "                  scoring = 'roc_auc',\n",
    "                  cv = cv,\n",
    "                  error_score = 'raise',\n",
    "                  n_jobs = -1)\n",
    "\n",
    "\n",
    "grids = [multiNB, rf, multiNB_tfidf, rf_tfidf] #rf\n",
    "\n",
    "grid_dict = {0: 'NB countvects',\n",
    "             1: 'rf countvects',\n",
    "             2: \"NB tfidf\",\n",
    "             3: 'rf tfidf'}\n",
    "\n",
    "print('Performing Model Optimizations')\n",
    "best_auc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best params are : %s' % gs.best_params_)\n",
    "    # Best training data recall score\n",
    "    print('Best training AUC score: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    y_pred_proba = gs.predict_proba(X_test)[:,1]\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set AUC score for best params: %.3f ' % roc_auc_score(y_test, y_pred_proba))\n",
    "    #Plot ROC curve for each estimator\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test,y_pred_proba)\n",
    "    plt.plot([0,1],[0,1], linestyle ='--')\n",
    "    plt.plot(fpr, tpr, label = 'auc=%.3f' % auc)\n",
    "    plt.title(label = 'ROC Curve %s' % grid_dict[idx])\n",
    "    name = grid_dict[idx]\n",
    "    plt.savefig(name + \".png\")\n",
    "    plt.show()\n",
    "    # Track best (highest test auc) model\n",
    "    if roc_auc_score(y_test, y_pred_proba) > best_auc:\n",
    "        best_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\nClassifier with best test set AUC: %s' % grid_dict[best_clf])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1_scores = pd.DataFrame(rf.cv_results_).sort_values(by = 'rank_test_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
    "X_train_vec_df = pd.DataFrame(X_train_vec)\n",
    "\n",
    "X_test_vec = vectorizer.transform(X_test).toarray()\n",
    "X_test_vec_df = pd.DataFrame(X_test_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_vec_df.columns = words\n",
    "#X_test_vec_df.columns = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf_fit = clf.fit(X_train_vec_df, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from scipy import interpolate\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_fit = pipe_multi_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "shap.initjs()\n",
    "explainer = shap.explainers.Permutation(clf_fit.predict_proba, X_train_vec_df, max_evals = 3000,\n",
    "                           feature_names = feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_test_vec_df[:100])\n",
    "shap_values = shap_values[...,1]\n",
    "\n",
    "np.shape(shap_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0], X_test_vec_df.iloc[:1000,:], feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return clf_fit.predict([X[:,i] for i in frange(X.shape[1])]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(f, X.iloc[:50,:]), feature_names = words)\n",
    "shap_values = explainer.shap_values(X.iloc[299,:], nsamples = 500)\n",
    "shap.force_plot(explainer.expected_value, shap_values, X_display.iloc[299,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "shap.initjs()\n",
    "shap_values1 = copy.deepcopy(shap_values)\n",
    "shap_values1.values = shap_values1.values[:,:,1]\n",
    "shap_values1.base_values = shap_values1.base_values[:,1]\n",
    "\n",
    "shap.plots.beeswarm(shap_values1, max_display = 20, show= False)\n",
    "plt.savefig('beeswarm.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "ind = 48\n",
    "print(X_test.iloc[ind], y_test.iloc[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[ind,:,1], show = True)\n",
    "plt.savefig('waterfall.png', bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:,words.tolist().index('story'),1], show = True)\n",
    "\n",
    "plt.savefig('scatter.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = pd.Series(clf_fit.feature_importances_,\n",
    "                          index = words).sort_values(ascending=False)\n",
    "feature_scores[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_pred_proba = clf_fit.predict_proba(X_test_vec_df)[:,1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold = %f, F-Score = %.3f' % (thresholds[ix], fscore[ix]))\n",
    "plt.plot(recall, precision, label = 'auc=%.3f' % auc, marker = '.')\n",
    "plt.plot(recall[ix], precision[ix], marker = 'o', color = 'black', label = 'Best')\n",
    "plt.title(label = 'Precision Recall Curve')\n",
    "plt.savefig('prec_recall.png', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "optimal_thresh = thresholds[ix]\n",
    "optimal_prec = precision[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = feature_scores.nlargest(10).sort_values(ascending=True)\n",
    "feat_imp.plot(kind = 'barh', figsize = (10,10))\n",
    "plt.savefig('feat_imp.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = (y_pred_proba > optimal_thresh)\n",
    "conf_mat = confusion_matrix(y_test, y_pred.astype(int), labels = [1,0])\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "y_pred_df = y_pred.astype(int)\n",
    "X_variables = pd.DataFrame(X_test)\n",
    "X_variables['predictions'] = y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews = X_variables[X_variables['predictions'] == 1 ]\n",
    "negative_reviews.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_variables.iloc[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unused Code For Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_data = pd.concat([X_train, y_train], axis = 1)\n",
    "nn_train_data.value_counts('recommended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_balance = nn_train_data[nn_train_data['recommended']==0].sample(4198, random_state = 48)\n",
    "nn_balance.value_counts('recommended')\n",
    "nn_train = pd.concat([nn_balance, nn_train_data[nn_train_data['recommended']==1]])\n",
    "nn_train.value_counts('recommended')\n",
    "nn_xtrain = nn_train['reviews_fin']\n",
    "nn_ytrain = nn_train['recommended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_arr = vectorizer.fit_transform(nn_xtrain).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_arr = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_arr.shape, X_test_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = X_train_arr.shape[1]\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_ytrain.value_counts()\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_shape = (n_words,), activation = 'relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation ='sigmoid'))\n",
    "\n",
    "tensorBoardCallback = TensorBoard(log_dir = './logs', write_graph = True)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "history = model.fit(X_train_arr, nn_ytrain, epochs = 10, callbacks=[tensorBoardCallback], verbose = 2,\n",
    "                   validation_data = (X_test_arr, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test_arr, y_test, verbose = 0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend([\"Train\", \"test\"], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend([\"Train\", \"test\"], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.shape, recall.shape, thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = np.delete(precision, -1)\n",
    "recall = np.delete(recall, -1)\n",
    "\n",
    "precision.shape, recall.shape, thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate recall from selected precision\n",
    "\n",
    "precision_score = optimal_prec\n",
    "find_recall = interpolate.interp1d(precision, recall)\n",
    "find_thresh = interpolate.interp1d(precision, thresholds)\n",
    "thresh_value = find_thresh(precision_score)\n",
    "recall_score = find_recall(precision_score)\n",
    "recall_score, thresh_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision, label = 'auc=%.3f' % auc, marker = '.')\n",
    "plt.plot(recall_score, precision_score, marker = 'o', color = 'black', label = 'Best')\n",
    "plt.title(label = 'Precision Recall Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_opt = find_tpr(thresh_value)\n",
    "fpr_opt = find_fpr(thresh_value)\n",
    "print(tpr_opt, fpr_opt, thresh_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating optimal point\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "find_tpr = interpolate.interp1d(thresholds, tpr) #plug in thresholds for tpr\n",
    "find_fpr = interpolate.interp1d(thresholds, fpr) #plug in thresh for fpr\n",
    "thresh_look = interpolate.interp1d(tpr, thresholds) #plug in tpr for thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = thresh_look(0.6) #threshold value for value of tpr\n",
    "ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc = roc_auc_score(y_test,y_pred_proba)\n",
    "plt.plot([0,1],[0,1], linestyle ='--')\n",
    "plt.plot(fpr, tpr, label = 'auc=%.3f' % auc, marker = '.')\n",
    "plt.plot(fpr_opt, tpr_opt, marker = 'o', color = 'black')\n",
    "plt.title(label = 'ROC Curve Random Forest Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_opt = (y_pred_proba > ttt)\n",
    "conf_mat_opt = confusion_matrix(y_test, y_pred_opt, labels = [0,1])\n",
    "conf_mat_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best true negative (tn) results\n",
    "tn = 7000\n",
    "best_thresh = 0\n",
    "for t in high_thresh:\n",
    "    y_pred_opt = (y_pred_proba > t)\n",
    "    conf_mat_opt = confusion_matrix(y_test, y_pred_opt, labels = [0,1])\n",
    "    if conf_mat_opt[1][0] > tn:\n",
    "        best_thresh = t\n",
    "        tn = conf_mat_opt[1][0]\n",
    "        conf_mat_best = conf_mat_opt\n",
    "\n",
    "best_thresh, tn, conf_mat_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_thresh = thresholds[thresholds > 0.5]\n",
    "high_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#accuracy prior to choosing threshold\n",
    "accuracy = accuracy_score(y_test, y_pred_proba.astype('int'))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred_proba\n",
    "y_pred = (y_pred >= opt_thresh).astype('int')\n",
    "\n",
    "accuracy_thresh = accuracy_score(y_test, y_pred)\n",
    "print(accuracy_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision, label = 'auc=%.3f' % auc, marker = '.')\n",
    "plt.plot(0.9, prec_point, marker = 'o', color = 'black', label = 'Best')\n",
    "plt.title(label = 'Precision Recall Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
